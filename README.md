# Intelligent-System-Projects
Contains all the Intelligent System Projects I have done.

## Projects

This folder contains all the python source code, with the outputs. It contains hierarchy of sub-folders, each containing a specific project with all its resources. 

To view a particular project, please open the folder. If you wish to run the project, kindly download the entire folder and run the project with all its resources kept in the same folder.

## Project Descriptions

1. Simulation of Different Types of Neurons (Izhikevich Model) - It uses the neurodynamical model, called Izhikevich model, to simulate three different neuron receptors and plots their behoaviour.

2. Simulation of Pre and Post Synpatically Gated Hebb Rules - It uses Gated-Hebb rule model to train a neuron, record its performance and then predicts on the MNIST dataset.

3. Perceptron Classification on partial MNIST Dataset - It trains a perceptron model, record its performance and plots a confusion matrix for 0 & 1 labels, on the MNIST dataset.

4. Neural Networks with Backpropagation (Generic) - It trains a neural network model using backpropagation, record its performance and plots a confusion matrix, on the MNIST dataset. It is a more generic implementation.

5. Neural Networks with Backpropagation (Optimized) - It trains a neural network model using backpropagation, record its performance and plots a confusion matrix, on the MNIST dataset. It is a more specific implementation with more optimization for performance.

6. Autoencoder with MNIST Dataset (Generic) - It trains an autoencoder model, record its performance and plots a confusion matrix, on the MNIST dataset. It is a more generic implementation.

7. Autoencoder with MNIST Dataset (Optimized) - It trains an autoencoder model, record its performance and plots a confusion matrix, on the MNIST dataset. It is a more specific implementation with more optimization for performance.

8. Autoencoder as a pretext for classification (Generic) - It trains a neural network model for classification based on an autoencoder trained as a pretext problem. It then trains and compares two models:

	a. Training just the output layer, with hidden layer as autoencoder.

	b. Training both the output and hidden layer, with hidden layer as autoencoder. 

Finally, it record their performance and plots confusion matrices of training and testing sets, on the MNIST dataset. It is a more generic implementation.

8. Autoencoder as a pretext for classification (Optimized) - It trains a neural network model for classification based on an autoencoder trained as a pretext problem. It then trains and compares two models:

	a. Training just the output layer, with hidden layer as autoencoder.

	b. Training both the output and hidden layer, with hidden layer as autoencoder. 

Finally, it record their performance and plots confusion matrices of training and testing sets, on the MNIST dataset. It is a more specific implementation with more optimization for performance.
